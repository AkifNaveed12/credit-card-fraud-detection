{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Credit Card Fraud Detection\n",
        "Rule-based ‚Üí Machine Learning approach\n"
      ],
      "metadata": {
        "id": "bLydGHoQFyEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "DZtRJdMrF_Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.listdir()\n"
      ],
      "metadata": {
        "id": "an-avEQrHXwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /creditcard.csv /content/\n"
      ],
      "metadata": {
        "id": "_9GyWEd4IRL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls /content\n"
      ],
      "metadata": {
        "id": "47gtbfO_Hxkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"creditcard.csv\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4uJg2OpzGckr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n",
        "\n",
        "# analyzing the dataset here rows will be the no of\n",
        "# transactions and columns will be be features + target"
      ],
      "metadata": {
        "id": "hhlVpyXFJ4Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n"
      ],
      "metadata": {
        "id": "iV3NUfpQLsa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n"
      ],
      "metadata": {
        "id": "o8IuxP3sKLvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Values Analysis\n",
        "\n",
        "The dataset contains no missing (null) values across all features, as confirmed using `df.info()`.\n",
        "Therefore, no imputation or missing-value handling is required for this dataset.\n"
      ],
      "metadata": {
        "id": "-Oc6nVJYLP0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Class'].value_counts()\n",
        "df['Class'].value_counts(normalize=True) * 100"
      ],
      "metadata": {
        "id": "K_IL7oPWMFKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class Imbalance Analysis\n",
        "\n",
        "The target variable `Class` shows a highly imbalanced distribution, where fraudulent transactions (Class = 1) represent a very small percentage of the total dataset, while non-fraudulent transactions (Class = 0) dominate.\n",
        "\n",
        "This imbalance makes accuracy an unreliable metric, as a model could predict all transactions as non-fraud and still achieve high accuracy. Therefore, evaluation metrics such as precision, recall, and F1-score are more appropriate for this problem.\n"
      ],
      "metadata": {
        "id": "F61w9KJgN6cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#now visulization of the percentage distributions of the target \"class\"\n",
        "\n",
        "sns.countplot(x='Class', data=df)\n",
        "plt.title('class distribution (Fraud vs Non Fraud)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WmmtpBmURmRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df[df['Class']== 0]['Amount'], bins=50, log_scale=True, label='Non-Fraud', color='blue')\n",
        "sns.histplot(df[df['Class']== 1]['Amount'], bins=50, log_scale=True, label = 'Fraud', color='red')\n",
        "plt.title('Distribution of Transaction Amounts (Fraud vs Non-Fraud)')\n",
        "plt.xlabel('Transaction Amount')\n",
        "plt.ylabel('Frequency -> (log scale)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VmXsKAVAT-ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the transaction amount distribution, we observe a highly right-skewed pattern with a long tail of extreme values. This indicates the presence of outliers and non-normality, which can negatively affect many machine learning models if not handled properly. The log scale helps reveal structure across different magnitudes. While amount alone does not clearly separate fraud from non-fraud, it remains an important supporting feature when combined with others."
      ],
      "metadata": {
        "id": "mKwg-Q4xZJsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(x='Class', y='Amount', data=df)\n",
        "plt.yscale('log')\n",
        "plt.title('Distribution of Transaction Amounts by Class (log scale)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q9P8OA4OZO4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do fraud transactions generally involve higher amounts?\n",
        "Answer:  ‚ùåNo ‚Äî median is lower\n",
        "\n",
        "Are fraud transactions more variable?\n",
        "Answer: ‚úÖ Yes ‚Äî larger spread\n",
        "\n",
        "Can amount alone classify fraud?\n",
        "Answer: ‚ùå No ‚Äî heavy overlap\n",
        "\n",
        "Is amount still useful as a feature?\n",
        "Answer: ‚úÖ Yes ‚Äî but after transformation and with other features\n",
        "\n",
        "*******\n",
        "Transaction amount is skewed, contains outliers, and overlaps across classes. Therefore, we should transform it (log), avoid rule-based thresholds, and combine it with other features using models robust to imbalance."
      ],
      "metadata": {
        "id": "dDqWRWW0btVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#feature engineering ->creating a transformed class\n",
        "df['Log_Amount'] = np.log1p(df['Amount'])\n"
      ],
      "metadata": {
        "id": "xFQ4Py8Ucb8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Amount', 'Log_Amount']].describe()\n"
      ],
      "metadata": {
        "id": "h34HT-ccjUST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df[df['Class']==0]['Log_Amount'], bins=50, label='Non-Fraud', log_scale=False)\n",
        "sns.histplot(df[df['Class']==1]['Log_Amount'], bins=50, label='Fraud', color='red', log_scale=False)\n",
        "plt.legend()\n",
        "plt.title(\"Log_Amount Distribution by Class\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_Vs4eNv8jv8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is Log_Amount alone sufficient to classify fraud?\n",
        "\n",
        "Answer: ‚ùå No\n",
        "But it helps models learn patterns better when combined with others."
      ],
      "metadata": {
        "id": "ygLQ6xpvkJ3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepration for modeling\n",
        "X= df.drop(columns=['Class'])\n",
        "y=df['Class']"
      ],
      "metadata": {
        "id": "fFQkSvx4kX_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we will be using the train test split as the imbalance is extreme!\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "CWdCJlIUkodV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "UXXObhaYo301"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BASE MODEL"
      ],
      "metadata": {
        "id": "XFyrt5WdxESN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "baseline_model = LogisticRegression(\n",
        "    class_weight='balanced',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "baseline_model.fit(X_train_scaled, y_train)\n"
      ],
      "metadata": {
        "id": "lmfzrtCHt8ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = baseline_model.predict(X_test_scaled)\n"
      ],
      "metadata": {
        "id": "n8v5gvbzuGqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model  evaluation\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "cj1Nwou9uZwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure()\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Logistic Regression Baseline\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uOC-T1FqvsPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TN | FP\n",
        "_______\n",
        "FN | TP\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   TP ‚Üí fraud caught ‚úÖ\n",
        "*   FN ‚Üí fraud missed ‚ùå\n",
        "*   FP ‚Üí false alarm\n",
        "*   TN ‚Üí correct non-fraud\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QnhrRL4dwF4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Probability Threshold Tuning\n",
        "y_proba = baseline_model.predict_proba(X_test_scaled)[:, 1]\n"
      ],
      "metadata": {
        "id": "91jMrigxx25y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "threshold = 0.3\n",
        "y_pred_custom = (y_proba >= threshold).astype(int)\n",
        "#Instead of predictions, get probabilities"
      ],
      "metadata": {
        "id": "6x_izpj7x6uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred_custom))\n"
      ],
      "metadata": {
        "id": "YWlpWxAmyE7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision‚ÄìRecall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sMsaIxGFy2sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A MORE STRINGER MODEL"
      ],
      "metadata": {
        "id": "SFvVb7gPzFk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "kHM8KZmNzJWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "EbJ3Rzoaz0A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model          | Recall (Fraud) | Precision | F1 |\n",
        "| -------------- | -------------- | --------- | -- |\n",
        "| Logistic (0.5) | ‚ùå              | ‚ùå         | ‚ùå  |\n",
        "| Logistic (0.3) | ‚úÖ              | ‚ö†Ô∏è        | ‚úÖ  |\n",
        "| Random Forest  | üî•             | üî•        | üî• |\n"
      ],
      "metadata": {
        "id": "TszBMfn5z4oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "using SMOTE to Handle Imbalance"
      ],
      "metadata": {
        "id": "7QAfvfts_vN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "qng2h25P_XED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y_train_smote).value_counts()"
      ],
      "metadata": {
        "id": "5kkmoIYN_n1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model on SMOTE Data**"
      ],
      "metadata": {
        "id": "bHkl9vapANnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1)**  Logistic Regression on SMOTE"
      ],
      "metadata": {
        "id": "4kfAb3tsAYW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_smote = LogisticRegression(max_iter=1000)\n",
        "lr_smote.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "y_pred_lr_smote = lr_smote.predict(X_test)\n"
      ],
      "metadata": {
        "id": "MDCyydf-AM6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred_lr_smote))\n"
      ],
      "metadata": {
        "id": "3tIutOD8BMHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2)**  Random Forest (SMOTE)"
      ],
      "metadata": {
        "id": "07J0U997Bdq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_smote = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_smote.fit(X_train_smote, y_train_smote)\n",
        "y_pred_rf_smote = rf_smote.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_rf_smote))\n"
      ],
      "metadata": {
        "id": "d8rFIj3sBsBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Comparison & Hyper parameter Tuning**"
      ],
      "metadata": {
        "id": "JgUN_oGAFjoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "def evaluate_model(name, model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "id": "7nJKBFF8GMLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logistic regression (Smote)\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "evaluate_model(\"Logistic Regression (SMOTE)\", lr, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "NDKURuCvGTs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gradient boosting\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "evaluate_model(\"Gradient Boosting\", gb, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "pW7Y8y6QGl0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "evaluate_model(\"Random Forest (SMOTE)\", rf, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "IxHOMGe9Ms3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model                         | Precision | Recall   | F1   | ROC-AUC   |\n",
        "| ----------------------------- | --------- | -------- | ---- | --------- |\n",
        "| Logistic Regression           | low       | low      | low  | baseline  |\n",
        "| Random Forest (SMOTE)         | **0.82**  | **0.70** | 0.76 | 0.956     |\n",
        "| **Gradient Boosting (SMOTE)** | **0.13**  | **0.88** | 0.23 | **0.977** |\n"
      ],
      "metadata": {
        "id": "hzB8_9bwQPSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting**\n",
        "\n",
        "Best recall ‚Üí catches most frauds\n",
        "\n",
        "Best ROC-AUC ‚Üí best ranking ability\n",
        "\n",
        "Lower precision ‚Üí more false alarms (acceptable in fraud)\n",
        "\n",
        "üëâ In fraud detection:\n",
        "\n",
        "**Missing a fraud is worse than flagging a normal transaction Gradient Boosting is a defensible final choice**"
      ],
      "metadata": {
        "id": "JZlJQLDKQb8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Selection Summary\n",
        "\n",
        "Three models were evaluated: Logistic Regression, Random Forest, and Gradient Boosting.\n",
        "Due to extreme class imbalance, recall and ROC-AUC were prioritized over accuracy.\n",
        "\n",
        "Gradient Boosting achieved the highest fraud recall (0.88) and ROC-AUC (0.97),\n",
        "making it the preferred model for this problem despite lower precision.\n",
        "Hyperparameter tuning was not performed due to computational constraints and\n",
        "is considered future work.\n"
      ],
      "metadata": {
        "id": "65056TcuQz7c"
      }
    }
  ]
}